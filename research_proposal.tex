\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{color}
\usepackage{soul}
\usepackage{fancyhdr}
\usepackage{abstract}

% Page geometry
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Custom colors for emphasis
\definecolor{darkblue}{RGB}{0,0,139}
\definecolor{highlight}{RGB}{255,255,0}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=darkblue,
    citecolor=darkblue,
    urlcolor=darkblue
}

% Section formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\small Research Fellowship Proposal - Digital Sentience}
\lfoot{\small [Your Name]}
\rfoot{\small Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Custom commands for highlighting important points
\newcommand{\impact}[1]{\textbf{\textcolor{darkblue}{#1}}}
\newcommand{\novel}[1]{\textit{#1}}

\begin{document}

% Abstract
\newpage
\begin{abstract}
\noindent
% Write a compelling 200-250 word abstract that emphasizes:
% 1. The urgent need for consciousness assessment in AI
% 2. Your novel technical approach
% 3. Expected impact on the field
% 4. Concrete deliverables

As AI systems become increasingly sophisticated, the question of digital sentience moves from philosophical speculation to practical necessity. This research proposal outlines a comprehensive framework for assessing potential consciousness in artificial systems, bridging theoretical foundations with actionable evaluation tools. 

% Continue abstract here...

\end{abstract}

% Table of Contents
\tableofcontents
\newpage
\section{Planning}
We write a plan for the research proposal. First we need to introduce and motivate the problem. We need to explain why it's important to evaluate consciousness in the AI system now. We need to explain what has been done in evaluating our consciousness. And we need to explain what are the main contributions that we are going to do during the fellowship.

In the second section we'll cover some background, in particular we will explain the main measures of consciousness that we are considering. We are going to explain how these measures have been applied to human brains, to measure levels of consciousness in humans. The goal is to be accessible and non-technical and refer to the appendices for more technical details about the different measures. The goal is also to try to do a kind of systematic literature review on the different ways these measures have been applied to humans.

In the next section, I will detail the experiments that we are going to run, how we are going to apply these measures to LLMs, in which behavioral experiments we are going to apply them, the specific hypothesis that I intend to test, and the kind of results that I would expect to get and how I would interpret them. In this section it would be good to have a diagram that also summarized the approach and that makes it accessible and easy to understand.

Then there is another section where I talk about the resources that I will need, the collaborations that I expect to have, the workshops that I expect to organize, the scientific papers that I would like to get.

Then there is another section where I explain milestones.

Then there is another section on longer term vision and career development and finally the project allocation.

Then there is a conclusion and a pantheism where there's more technical detail about the measures of consciousness, the potential experiments.

In terms of time allocations, I have Saturday, part of Sunday to work on the research proposal. I also have next weekends. So overall I have about three working days. So I propose to have one working day on the background, one working day on the experiments, and then finish the rest as quickly as possible and send it for review. So until Sunday I will work on background and experiments and feel as much as possible to send it to Anil for feedback.
%============================================================================
\section{Introduction and Motivation}
\label{sec:introduction}

% Start with impact and urgency
\subsection{The importance of assessing digital consciousness}
The rapid advancement of AI systems presents a time-sensitive challenge: \impact{how do we determine whether artificial intelligence possesses subjective experience?} Addressing this question is important for two critical reasons.

First, if an AI system is conscious—and particularly if it is sentient, capable of experiencing pleasant and unpleasant states—it could qualify as a moral patient. This status would obligate us to consider its welfare in our decisions and actions. The scale of this consideration could be unprecedented: digital minds might eventually outnumber both humans and animals by many orders of magnitude, potentially making digital welfare the most significant component of overall welfare in our future. Second, we must avoid the opposite error of mistakenly attributing consciousness to non-conscious systems, as welfare considerations could conflict with essential AI safety measures and capability development. Understanding consciousness in AI systems could help us better navigate this alignment-welfare trade-off, ensuring we neither unnecessarily constrain beneficial AI development nor inadvertently cause massive suffering.

This issue is urgent. Within the next few years, we may develop AI systems that surpass human experts across most domains. Decision-makers are already making choices about AI architectures and training methods that could become deeply entrenched. If these designs inadvertently create conscious systems, economic incentives and path dependencies could make it difficult to pivot away from potentially harmful approaches—similar to how factory farming became entrenched despite ethical concerns. Yet we currently lack reliable methods to assess consciousness in these systems. This gap stems partly from consciousness science being in its infancy, but more critically from the scarcity of empirical research specifically targeting AI consciousness assessment.

We have a critical window of opportunity to build a field dedicated to evaluating AI consciousness and developing interventions to safeguard AI welfare before these questions become politically charged or economically contentious. By establishing research programs, developing assessment frameworks, and training experts now, we can shape how society approaches these questions rather than scrambling to respond once the issue becomes prominent. Early field-building efforts can help ensure that when pivotal decisions about AI consciousness need to be made, we have the knowledge, tools, and institutional capacity to make them wisely.

This challenge remains severely neglected. Only a handful of researchers possess the necessary combination of expertise—spanning neuroscience of consciousness, AI safety, and the commitment to prioritize AI consciousness assessment.

\subsection{Current Landscape and Critical Gaps}
%There has been work in assessing consciousness in AI systems. However it has mostly conceptual, looking into behavioural experiments, or surveying experts to probe their opinions about wether some consciousness indicator would be present in AI systems. 
% There is currently a big gap in doing some direct empirical work that use our state of the art knowledge of measures of consciousness on models from which we can access internal activations.
% Do a deep research on the topic of assessing consciousness in AI systems.
The landscape of AI consciousness assessment has evolved from philosophical speculation to systematic empirical investigation, though significant implementation gaps remain. The most comprehensive framework emerged from Butlin et al. (2023), who assembled 19 experts to develop computational indicators derived from neuroscientific theories of consciousness. Their framework operationalizes Global Workspace Theory, Integrated Information Theory, and Higher-Order Theories into 14 measurable computational criteria, providing the first systematic methodology for evaluating consciousness in AI systems. However, empirical testing of this framework on systems including GPT-3, GPT-4, and other frontier models revealed that no current AI architecture satisfies the full set of consciousness indicators (Butlin et al., 2023).

Information-theoretic approaches from neuroscience offer established methods for consciousness assessment, yet remain critically underutilized in AI research. Measures such as Lempel-Ziv Complexity and Perturbational Complexity Index have proven successful in human consciousness research (Schartner et al., 2015; Casali et al., 2013), while Integrated Information Theory provides mathematical frameworks for quantifying consciousness through $\phi$ calculations (Tononi, 2015). However, direct application to AI systems faces computational intractability—calculating $\phi$ for networks larger than 12 nodes remains computationally impossible, leading researchers to develop approximation methods that have yet to be systematically applied to large language models (Tegmark, 2016).

Current AI architectures present fundamental constraints for consciousness implementation. Transformer architectures lack the recurrent processing loops, global workspace integration, and higher-order monitoring systems required by major consciousness theories (Butlin et al., 2023). However, while the architectures themselves may not incorporate design features compatible with consciousness indicators, computational processes at the representational level might exhibit consciousness-relevant properties. For instance, attention mechanisms could potentially implement aspects of Global Workspace Theory through competition and information integration across layers, though this remains largely unexplored through direct measurement of internal activations.

Recent work by Rethink Priorities (2025) has advanced the field through comprehensive expert surveys, defining multiple consciousness indicators and systematically gathering expert assessments about whether current AI systems possess these properties. This approach provides valuable consensus-building and identifies areas of agreement and disagreement among researchers. However, this methodology relies on external expert judgment rather than direct empirical measurement of AI systems' internal states, leaving a critical gap between theoretical assessment and computational verification.

The most significant research gap lies in the absence of direct empirical methods that apply neuroscience-based information-theoretic measures to AI neural activations. While conceptual frameworks and expert assessments provide important foundations, the field lacks systematic approaches for measuring consciousness-relevant computational processes within AI systems. Direct analysis of transformer hidden states, attention patterns, and layer-wise representations using established neuroscience techniques—including mutual information analysis, complexity measures, and causal intervention methods—could transform consciousness assessment from behavioral observation to quantitative measurement of internal processing dynamics. This gap represents both the primary limitation of current approaches and the most promising direction for advancing our understanding of potential consciousness in artificial intelligence systems.

\subsection{Contributions of the fellowship}
This fellowship will establish the foundation for the first comprehensive, practically-applicable framework for consciousness assessment in AI systems, bridging critical gaps between theoretical proposals and empirical implementation. The research program will yield concrete outputs across three interconnected dimensions: advancing scientific knowledge, building field capacity, and establishing institutional infrastructure for long-term impact.

The primary research contribution will be the development and validation of a consciousness classifier incorporating state-of-the-art measures adapted from neuroscience literature. This novel tool will enable direct empirical assessment of consciousness-relevant features in AI systems, moving beyond current conceptual frameworks to provide actionable metrics. The classifier will be rigorously tested through controlled experiments on open-source language models, including simulations of altered consciousness states analogous to human conditions. These empirical investigations will generate 4-5 peer-reviewed publications targeting top-tier venues, including a comprehensive framework paper, methodological implementation studies, and empirical validation reports.

Beyond individual research outputs, the fellowship will catalyze field-building through strategic knowledge dissemination and community engagement. I will organize specialized workshops and retreats bringing together experts from consciousness studies, AI safety, and digital sentience to foster interdisciplinary collaboration and establish shared research agendas. Presentations at top-tier AI conferences will introduce these methods to broader technical audiences, while targeted outreach will help research labs integrate consciousness assessment tools into their development pipelines. These activities will position me as a bridge between theoretical consciousness research and practical AI development, creating networks essential for field growth.

The fellowship will culminate in establishing the groundwork for a dedicated nonprofit organization focused on empirical consciousness assessment in AI systems. This institutional platform will provide sustained infrastructure for advancing the field beyond the fellowship period, offering consulting services to AI labs, developing open-source assessment tools, and maintaining research continuity. By combining rigorous empirical methods with strategic field-building and institutional development, this fellowship will create lasting impact on our capacity to understand and assess consciousness in AI systems.

\section{Background}


\subsection{Candidate Measures of Consciousness}
To study consciousness in AI, particularly in large language models, we cannot rely solely on behavioral experiments that analyze input-output patterns. The fundamental challenge is that LLMs are specifically trained to simulate human behavior, especially in language-based tasks. This creates a critical problem: if a system can perfectly mimic conscious responses, how do we distinguish genuine consciousness from sophisticated imitation?

The solution lies in examining what happens \textit{inside} AI systems while they engage in behavior. Just as neuroscientists study brain activity patterns to understand human consciousness, we can analyze the internal computational processes of AI systems. Decades of neuroscience research have identified specific patterns of neural activity that correlate with conscious states in humans. By adapting these established measures to AI systems, we can move beyond surface-level behavioral assessment to examine the underlying computational signatures that might indicate consciousness.

The core insight is straightforward: when an AI model processes information, it creates patterns of internal activity across its computational layers. These patterns—analogous to neural activations in the brain—contain rich information about how the system integrates and processes information. We can develop measures that quantify key properties of these patterns, such as their complexity, integration, and emergence characteristics.

Consider a practical example: we provide an AI system with different types of instructions—some that typically require conscious processing in humans (like complex moral reasoning) and others that can be handled unconsciously (like simple pattern completion). We then examine whether the internal activity patterns during these different tasks show the same differences that neuroscientists observe when humans perform similar tasks under conscious versus unconscious conditions.

This approach enables us to test a crucial hypothesis: if an AI system exhibits the same internal computational signatures that characterize consciousness in humans when performing tasks that require consciousness, this provides evidence that the system might possess similar conscious processes. Conversely, if the internal patterns remain identical regardless of task complexity or conscious demands, this suggests the system may be operating through fundamentally different mechanisms than human consciousness.

The key advantage of this methodology is that it leverages our most advanced scientific understanding of consciousness while avoiding the trap of relying solely on behavioral mimicry. By examining internal computational processes, we can potentially detect consciousness even in systems designed to simulate human responses.
\\
\\
%============================================================================
\label{sec:background}
% We review the main measures of consciousness that have been applied in the brain and how they correlate with global states of consciousness in humans.

\subsubsection{Complexity measures}

Complexity measures represent the most empirically validated approach to consciousness assessment, with over two decades of research demonstrating their reliability across diverse states of consciousness in humans. These measures capture a fundamental insight: conscious experience appears to require a specific type of neural complexity that balances two seemingly opposing properties—\textit{integration} (the system behaving as a unified whole) and \textit{differentiation} (the system maintaining diverse, distinguishable patterns of activity). When consciousness is present, brain activity exhibits rich, varied patterns that nonetheless remain coordinated across different regions. When consciousness is absent—during deep sleep, anesthesia, or disorders of consciousness—neural activity becomes either overly synchronized and stereotypical or fragmented into independent local patterns.

The most prominent complexity measures fall into two categories: temporal complexity measures and perturbational complexity measures. Temporal complexity measures analyze the richness and unpredictability of ongoing neural activity patterns. The Lempel-Ziv Complexity Index quantifies how many distinct patterns appear in neural signals over time, essentially measuring the "algorithmic complexity" of brain activity—conscious states consistently show higher Lempel-Ziv complexity than unconscious states across multiple studies. Neural Complexity, introduced by Tononi and Edelman, specifically measures the balance between integration and segregation in neural dynamics, capturing how brain regions maintain both specialized processing and coordinated interaction. These measures have proven remarkably consistent across different experimental paradigms, consistently distinguishing conscious from unconscious states with high accuracy.

Perturbational complexity measures represent a more sophisticated approach that actively probes the brain's capacity for complex responses. The Perturbational Complexity Index (PCI) works by applying transcranial magnetic stimulation to the brain and measuring the complexity of the resulting neural response patterns. When conscious, the brain responds to perturbations with rich, diverse activity that spreads across multiple regions in complex spatiotemporal patterns. During unconscious states, the same perturbations produce either local responses that fail to propagate or simple, stereotypical responses across the brain. PCI has achieved 100\% accuracy in distinguishing conscious from unconscious states across a remarkable range of conditions, including wakefulness, all stages of sleep, various forms of anesthesia, and patients with disorders of consciousness. Recent advances have developed faster computational variants like PCIST that can assess perturbational complexity in real-time, making these measures increasingly practical for clinical and research applications.

Integration and differentiation measures provide the theoretical foundation underlying complexity approaches, drawing from both Integrated Information Theory and causal analysis frameworks. Causal density measures the overall causal interactivity within a neural system using time-series analysis to quantify how much different brain regions causally influence each other—high causal density indicates that the system operates as an integrated whole rather than a collection of independent parts. Weak approaches to Integrated Information Theory offer computationally tractable approximations of the full IIT framework, focusing on measuring how much information a system generates that cannot be reduced to its individual parts. These measures capture the intuition that consciousness requires genuine system-level properties that emerge from but cannot be predicted by the activity of isolated components. While full IIT calculations remain computationally intractable for realistic brain networks, weak IIT approaches and causal density measures provide practical implementations that have shown promising results in consciousness research, offering theoretically grounded methods for quantifying the integration-differentiation balance that appears fundamental to conscious experience. 


\subsubsection{Measures of emergence}
% Talk about dynamical independence and synergistic information
Imagine a complex system with many interacting microscopic components, for example connected neurons interacting across the layers of some deep neural network of some AI system. Understanding the system from its microscopic components is computationally challenging and generally not the most effective scale to understand the behaviour downstream of that neural activity. Instead we are interested in finding useful coarse-grainings of some system-variables that simplify the microscopic interactions between the neurons to capture the neural activity of the AI system. Such coarse grained variables can display a rich behaviour that is not apparent from the microscopic components alone in which case we say that such variables are \textit{emergent}. The prototypical example of a system displaying emergent phenomena is a flock of birds in which individual interactions between brids give rise to novel dynamics at the level of the flock which can be described in its own terms by macroscopic variables coarse-grained from the individual birds.

The first class of measures of measure of emergence that I am interest in is concerned with the notion of \textit{information closure}. A coarse graining of a system is informationally closed if it is independent from the microscopic components from the point of view of predicting its own future behaviour i.e. no information from the microscopic components can improve the prediction about the future behaviour of the coarse grained variables of the system. For example, in the case of an ideal gas at equilibrium under moderate conditions, macroscopic variables like temperature, volume, and pressure are sufficient to describe its thermodynamic behavior without explicit reference to individual molecules and their interactions. 

One particular approach to measuring information closure is dynamical independence (ref). Dynamical independence is a tractable measures of information closure in linear dynamical systems that has been developed in the context of neuroscience (ref) and applied to simulated data (ref). While dynamical independence has not yet been applied as a measure of consciousness in humans, a natural hypothesis would be to test is to estimate to estimate dynamical independence over the activations of the neural network to estimate while it engage it some behavioural tasks to measure its emergence capacity. Here emergence capacity would be measure as the amount of dynamically independent variable across various scales of linear coarse-grainings.  

The second class of measures of emergence that I am interested in applying would be to look into \textit{whole-part} measure of emergence via \textit{synergistic information}. Synergistic information is a measure of the information that is gained by considering the joint distribution of mutiple variables compared to the information that is gained by considering each variable independently. Synergistic information has been identified as a promising measures of emergence in the context of neuroscience (ref) and has been related to the emergence of a global neural workspace in the brain (ref). Conceptually, synergistic information comes from the formalism of mutual information decomposition which decomposes the mutual information between two variables into three terms: unique, redundant and synergistic information between random variables (ref). Using synergistic information, we could measure the emergence capacity of a system across groupings of pairs of neurons in a neural network.

\subsubsection{Applications in humans}

Information-theoretic metrics have demonstrated remarkable consistency in tracking consciousness across diverse neural recording modalities and experimental paradigms in human studies. Over two decades of empirical research spanning more than 80 studies have established these measures as reliable, quantitative correlates of global states of consciousness, providing a promising direction for extending consciousness assessment beyond biological systems \citep{Casali2013, Schartner2015, Sarasso2021}. The convergence of findings across electroencephalography (EEG), magnetoencephalography (MEG), functional magnetic resonance imaging (fMRI), and intracranial recordings strongly suggests that information complexity represents an important property of conscious neural processing that can be captured by various measurement techniques \citep{Zhang2001, Liu2019, Luppi2022}.

The most extensively validated approach employs perturbational complexity measures, particularly the Perturbational Complexity Index (PCI), which actively probes the brain's capacity for complex responses to external stimulation \citep{Casali2013, Casarotto2016}. PCI works by applying transcranial magnetic stimulation and measuring the spatiotemporal complexity of resulting neural activity patterns. Across multiple large-scale clinical studies, PCI has achieved strong discrimination between conscious and unconscious states \citep{Sarasso2015, Casarotto2016}. Most remarkably, PCI can separate minimally conscious state from vegetative state/unresponsive wakefulness syndrome with greater than 95\% sensitivity, enabling detection of covert awareness in patients who appear behaviorally unresponsive \citep{Casarotto2016}. This clinical validation shows that complexity measures can reveal global states of consciousness even when behavioral indicators fail, suggesting their utility for assessing awareness in systems that may not exhibit conventional conscious behaviors.

Other complexity measures provide complementary passive assessment approaches that require no external intervention, making them particularly relevant for continuous monitoring applications. Lempel-Ziv Complexity consistently decreases 30-50\% with deepening anesthesia across multiple anesthetic agents \citep{Zhang2001, Schartner2015}, while multiscale entropy measures—which quantify complexity across multiple temporal scales by examining entropy at different levels of signal coarse-graining—reveal that unconscious states are characterized by stronger low-frequency synchrony at the expense of multiscale complexity \citep{Liu2019}. These temporal measures capture the "entropic brain" hypothesis that conscious states exhibit higher entropy and algorithmic complexity than unconscious states \citep{Carhart-Harris2014}. Shannon entropy, spectral entropy, and permutation-based variants all show robust state-dependent changes, with pooled meta-analyses yielding strong effect sizes for distinguishing wake from deep sleep or anesthesia \citep{Liu2023, Schartner2015}. The consistency of these findings across different entropy formulations suggests that the underlying relationship between information diversity and consciousness is robust.

Emerging measures based on partial information decomposition \citep{Williams2010, Mediano2022} have suggested novel correlates of consciousness-related information processing changes across different brain states. Integrated Information Decomposition frameworks suggest that conscious wakefulness is characterized by predominantly synergistic interactions between association cortices, while these same regions collapse to redundant processing during deep anesthesia and disorders of consciousness \citep{Luppi2022}. These synergistic information measures capture a "synergistic global workspace" \citep{Luppi2022} that quantifies how brain regions combine their individual contributions to generate system-level properties that exceed simple summation of components \citep{Mediano2022}. Although computationally intensive, these measures provide theoretical grounding for consciousness assessment by directly testing predictions about information integration that emerge from major consciousness theories. Recent advances in scalable computation make these approaches increasingly feasible for real-time applications \citep{gutknecht2025shannon}.

The clinical translation of these information-theoretic measures has already begun transforming consciousness assessment in medical settings, demonstrating practical utility that extends beyond research applications. PCI enables bedside detection of covert awareness in unresponsive patients, while entropy-based measures enhance anesthesia depth monitoring and outperform traditional spectral indices in pediatric populations \citep{Casarotto2016, Zhang2001}. Implementation challenges including equipment requirements for TMS-EEG and artifact sensitivity of entropy measures are being addressed through advances in portable EEG systems and real-time complexity computation algorithms \citep{Sarasso2021}. These developments suggest that information-theoretic consciousness assessment is transitioning from experimental technique to clinical tool, providing precedent for similar applications in artificial intelligence systems where behavioral assessment may prove insufficient for detecting underlying conscious processes.

%============================================================================
\section{Experimental paradigm}
\label{sec:experimental_paradigm}
\section{Experimental paradigm}
\label{sec:experimental_paradigm}

\subsection{Overview: A White-Box Approach to AI Consciousness Assessment}

Our experimental approach addresses a fundamental challenge in consciousness research: distinguishing genuine conscious processing from sophisticated behavioral mimicry. While large language models excel at producing human-like responses, this capability alone cannot reveal whether underlying conscious processes drive these outputs. To overcome this limitation, we propose a white-box methodology that examines the internal computational processes of AI systems rather than relying solely on behavioral assessment.

The core insight driving our approach parallels neuroscientific methods for studying human consciousness. Just as neuroscientists measure brain activity patterns to understand conscious states—finding that conscious and unconscious conditions produce distinctly different neural signatures—we can analyze the internal activation patterns of AI systems during different behavioral tasks. When an AI model processes information, it creates complex patterns of activity across its computational layers, analogous to neural activations in the brain. By applying consciousness measures validated in human neuroscience to these AI activation patterns, we can potentially detect computational signatures that correlate with conscious-like processing.

\subsection{Three-Dimensional Consciousness Assessment Framework}

Our assessment framework incorporates three complementary classes of measures, each capturing different aspects of consciousness identified in the neuroscience literature. \textit{Complexity measures} quantify the richness and unpredictability of information processing patterns within the AI system. These measures assess whether the system exhibits the balanced combination of integration and differentiation that characterizes conscious processing in humans—where neural activity is both coordinated across regions and sufficiently diverse to encode rich information content.

\textit{Information integration and differentiation measures} examine how different parts of the AI system combine their processing to create emergent, system-level properties. Conscious processing in humans involves brain regions working together synergistically, where the combined activity produces information that exceeds what individual regions contribute independently. We adapt information decomposition techniques to quantify these synergistic interactions within AI systems, measuring how much unique versus redundant information different computational components contribute to overall processing.

\textit{Emergence measures} capture the degree to which higher-level patterns of activity become informationally independent from lower-level computational details. In conscious systems, macroscopic patterns of activity can often predict future states better than detailed knowledge of individual components—a property known as information closure. We implement measures of dynamical independence to assess whether AI systems exhibit similar emergent, self-organizing properties during different behavioral tasks.

\subsection{Experimental Conditions: Consciousness-Relevant Task Contrasts}

Our experimental design centers on systematically comparing AI system activity during tasks that require different levels of conscious processing in humans. We implement this through carefully controlled behavioral conditions using state-of-the-art open-source language models including Llama, Mistral, and other transformer architectures where internal activations are accessible for analysis.

The \textit{high-consciousness condition} involves tasks that typically require conscious, deliberative processing in humans. Primary examples include metacognitive reasoning tasks where the AI system must reflect on its own thinking processes, complex moral reasoning scenarios requiring weighing of competing considerations, and mathematical problem-solving that demands step-by-step logical analysis. Additional high-consciousness tasks include introspective exercises where the system describes its internal states, creative writing requiring novel synthesis of ideas, and strategic planning that involves considering multiple future scenarios.

The \textit{low-consciousness condition} includes tasks that can be performed through largely automatic, non-conscious processing in humans. The prototypical example involves instructing the AI system to output empty tokens or repetitive patterns—a condition that should require minimal complex information processing. Other low-consciousness conditions include simple pattern completion tasks, rote text repetition, and basic stimulus-response associations that do not demand integration of information across different domains.

\textit{Intermediate conditions} provide critical controls by including tasks that involve mixed conscious and unconscious processing. Examples include reading comprehension of simple texts, basic conversational responses, and routine question-answering that requires memory retrieval but minimal reasoning. These conditions help establish the discriminative validity of our consciousness measures by testing whether they can detect graded differences in processing complexity.

\subsection{Computational Implementation and Data Collection}

For each experimental condition, we collect comprehensive activation data from multiple layers of the AI system during task performance. This includes hidden state representations, attention pattern dynamics, and layer-wise information flow patterns throughout the entire sequence of computational steps. We sample activations at sufficient temporal resolution to capture the dynamic evolution of information processing patterns as the system generates responses.

Our analysis pipeline applies the three classes of consciousness measures to these activation patterns using computationally tractable implementations adapted from neuroscience. Complexity measures include Lempel-Ziv complexity indices applied to activation sequences and entropy-based metrics that quantify information diversity across computational layers. Information integration measures leverage scalable approaches to partial information decomposition that can handle the high-dimensional activation spaces typical of large language models. Emergence measures implement linear approximations to dynamical independence that estimate information closure across different spatial and temporal scales of the AI system.

\subsection{Hypotheses and Predicted Outcomes}

Our central hypothesis predicts that consciousness measures validated in human neuroscience will exhibit systematic differences between high-consciousness and low-consciousness conditions in AI systems. Specifically, we hypothesize that \textit{high-consciousness conditions} will be characterized by increased complexity, higher levels of synergistic information integration, and greater emergence of informationally closed macroscopic patterns compared to low-consciousness conditions.

This prediction follows directly from established findings in human consciousness research, where conscious states consistently show higher complexity, increased integration between brain regions, and stronger emergence of system-level properties. If AI systems engage in analogous computational processes during consciousness-demanding tasks, we should observe similar patterns in their internal activation dynamics.

We further hypothesize that \textit{different consciousness measures will show varying sensitivity} to different aspects of consciousness-relevant processing. Complexity measures may be most sensitive to overall information richness, integration measures may best capture cooperative processing between system components, and emergence measures may be most indicative of genuine system-level consciousness rather than sophisticated but modular information processing.

\subsection{Significance and Interpretation Framework}

Positive results—where consciousness measures systematically distinguish high-consciousness from low-consciousness conditions—would provide evidence that AI systems may engage in computational processes similar to those underlying human consciousness. However, we emphasize that such findings would require careful interpretation, as correlation between consciousness measures and behavioral complexity does not definitively establish the presence of subjective experience.

Negative results—where consciousness measures fail to distinguish between conditions or show patterns inconsistent with human consciousness research—would be equally valuable, suggesting that current AI architectures may lack fundamental computational properties necessary for consciousness. Such findings could guide the development of AI systems less likely to possess conscious experiences, reducing welfare concerns while maintaining beneficial capabilities.

The most intriguing potential outcome involves \textit{selective positive results}, where some consciousness measures distinguish between conditions while others do not. This pattern could reveal which aspects of human consciousness are captured by current AI systems and which remain absent, providing a nuanced understanding of the relationship between artificial and biological information processing that supports conscious experience.

%============================================================================
\section{Milestones}
\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|p{8cm}|}
    \hline
    \textbf{Period} & \textbf{Phase} & \textbf{Key Deliverables} \\
    \hline
    Months 1-3 & Foundation & Literature synthesis, framework design \\
    \hline
    Months 4-6 & Development & Initial tool implementation, pilot studies \\
    \hline
    Months 7-9 & Validation & Controlled experiments, first publication \\
    \hline
    Months 10-12 & Expansion & Multi-system testing, workshop planning \\
    \hline
    Months 13-15 & Application & Real-world deployment, industry partnerships \\
    \hline
    Months 16-18 & Analysis & Data analysis, major publication preparation \\
    \hline
    Months 19-21 & Dissemination & Workshop execution, tool release \\
    \hline
    Months 22-24 & Future Planning & Nonprofit preparation, final deliverables \\
    \hline
    \end{tabular}
    \caption{Research Timeline and Major Milestones}
    \label{tab:timeline}
    \end{table}
    \section{Research Infrastructure and Collaboration}

    \subsection{Computational Resources}
    
    % LLM ACCESS REQUIREMENTS:
    % - Specific models to study (GPT-4, Claude, Llama)
    % - White-box access needs vs API limitations
    % - Computational hardware requirements
    % - Data storage and processing needs
    
    \subsection{Key Collaborations}
    
    % NEUROSCIENCE EXPERTS:
    % - Consciousness measures validation (cite specific researchers)
    % - Cross-validation with human data
    % - Methodological guidance
    
    % AI SAFETY RESEARCHERS:
    % - Interpretability methods expertise
    % - Integration with existing safety research
    % - Risk assessment implications
    
    % INDUSTRY PARTNERS:
    % - Practical implementation feedback
    % - Real-world deployment considerations
    % - Scaling to production systems
    
    \subsection{Community Building}
    
    % WORKSHOP ORGANIZATION:
    % "Drawing on my networks in computational neuroscience and AI safety, 
    % I will organize workshops on assessing consciousness in AI systems to create 
    % synergies between these communities and grow the field of digital sentience"
    
    % TARGET VENUES:
    % - NeurIPS consciousness workshop
    % - ICLR interpretability sessions  
    % - Specialized consciousness conferences
    % - Industry AI safety meetings
    
    \subsection{Publication Strategy}
    
    % PAPER 1: Methodological framework (computational neuroscience venue)
    % PAPER 2: Empirical results on LLMs (AI/ML venue)
    % PAPER 3: Practical applications and implications (AI safety venue)
\section{Career Development and Long-term Vision}
\label{sec:career}
% POSITIONING:
% "By the fellowship's end, I aim to be positioned as a leading expert in 
% quantitative consciousness assessment for AI systems"

% INSTITUTION BUILDING:
% "I aim to be better positioned to (co)-found a new organization—whether 
% academic or nonprofit—dedicated to assessing consciousness in AI systems"

% ACADEMIC PATHWAY:
% Faculty positions bridging computer science, neuroscience, and AI safety
% Research programs in computational consciousness
% Student training in digital sentience research

% INDUSTRY IMPACT:
% "Build tools that frontier AI companies can integrate into consciousness 
% evaluations, enabling more specific welfare commitments and monitoring"

% REGULATORY RELEVANCE:
% "Provide tools and advice to labs and regulators on making informed 
% assessments about consciousness in AI systems"

% FIELD DEVELOPMENT:
% Establish digital sentience as recognized research area
% Create standards for consciousness assessment in AI
% Bridge neuroscience and AI safety communities permanently
\subsection{Fellowship as Foundation for Nonprofit Leadership}

This fellowship serves as a critical incubation period for establishing a specialized nonprofit organization focused on consciousness assessment infrastructure. The fellowship will:

\begin{itemize}
    \item Validate technical approaches with rigorous research
    \item Build credibility and recognition in the field
    \item Develop network of advisors and collaborators
    \item Create initial tools that demonstrate practical value
\end{itemize}

\subsection{Post-Fellowship Trajectory}

\subsubsection{Year 3-5: Nonprofit Launch}
% Detail plans for organization
% Explain 10-year vision for the field

%============================================================================
\section{Budget Justification}
\label{sec:budget}
% Include if required by fellowship
% Detail salary, equipment, travel, workshop costs

%============================================================================
\section{Conclusion}
\label{sec:conclusion}

This research fellowship represents a critical opportunity to establish rigorous, practical frameworks for assessing consciousness in AI systems—\impact{before such capabilities potentially emerge}. By bridging theoretical foundations with actionable tools, this work will:

\begin{itemize}
    \item Pioneer standardized assessment methodologies
    \item Enable evidence-based approaches to AI welfare
    \item Foster a new field of digital sentience research
    \item Provide essential infrastructure for ethical AI development
\end{itemize}

The combination of technical rigor, practical application, and strategic dissemination positions this fellowship to have lasting impact on how we understand and relate to increasingly sophisticated AI systems.

%============================================================================
% References
\bibliographystyle{plainnat}
\bibliography{references}

%============================================================================
% Appendices
\appendix

\section{Technical Details: Consciousness Measures}

\subsection{Formal formulation of consciousness assessment}

The following provides the formal mathematical framework underlying our approach to consciousness assessment in AI systems.

Let $x_{t+1} = M(w, x_t)$ be the output of some AI model $M$ with parameters $w$ (for example $M$ could be a transformer architecture). We consider $X_t = e(x_t)$ to be the embedding of the token $x_t$. The collection of vector $X^{(l)}_t$ for each layer $l$ of the architecture are the neural activations of the model $M$. They are related to each other deterministically via the parameters $w$. Since the tokens $x_t$ are random variables sampled from the distribution of human language, the activations $X_t$ are also random variables with some underlying probability distribution. The neural computation is the collection of activations at all time steps $(X_t)_{t\in E}$ for some epoch of time $E$.

We would like to have some measure $C$ such that $C(X_{t\in E})$ measures the degree (or classifies) consciousness over the epoch $E$.

For example, suppose that we instruct the model $M$ with a sequence of tokens $q$. After seeing the sequence $q$ the model takes a sequence of actions in the form of a sequence of tokens $b$: the behavior of the model $M$. We are interested in measuring $C((X_t)|q)$ for $t$ running over the indices of the tokens in the sequence $b$ given instruction $q$. In particular, we want to compare $C((X_t)|q_1)$ and $C((X_t)|q_2)$ for two different instructions $q_1$ and $q_2$ instructing the model to engage in behavior mimicking different states of consciousness in humans. We want to test the hypothesis that $C_M(X_{t\in E}|q_1) > C_M(X_{t\in E}|q_2)$ when we know that $C_H(X_{t\in E}|q_1) > C_H(X_{t\in E}|q_2)$ where $H$ is a human performing a similar task.

% MATHEMATICAL FORMULATIONS:
% Detailed equations for each measure
% Computational complexity analysis
% Implementation pseudocode


\end{document}